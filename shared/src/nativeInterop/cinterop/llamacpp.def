headers = llama.h
headerFilter = llama.h
package = com.privateai.vault.llamacpp

# iOS ARM64 (iPhone/iPad)
compilerOpts.ios_arm64 = -Isrc/nativeInterop/cinterop/headers
linkerOpts.ios_arm64 = -Lsrc/nativeInterop/cinterop/libs/ios-arm64 -lllama -lc++ -framework Accelerate -framework Metal

# macOS ARM64 (Apple Silicon)
compilerOpts.macos_arm64 = -Isrc/nativeInterop/cinterop/headers
linkerOpts.macos_arm64 = -Lsrc/nativeInterop/cinterop/libs/macos-arm64 -lllama -lc++ -framework Accelerate -framework Metal

# macOS x64 (Intel Mac)
compilerOpts.macos_x64 = -Isrc/nativeInterop/cinterop/headers
linkerOpts.macos_x64 = -Lsrc/nativeInterop/cinterop/libs/macos-x64 -lllama -lc++ -framework Accelerate

# Windows x64 (MinGW)
compilerOpts.mingw_x64 = -Isrc/nativeInterop/cinterop/headers
linkerOpts.mingw_x64 = -Lsrc/nativeInterop/cinterop/libs/windows-x64 -lllama -lstdc++

# Static libraries to link
staticLibraries = libllama.a
libraryPaths.ios_arm64 = src/nativeInterop/cinterop/libs/ios-arm64
libraryPaths.macos_arm64 = src/nativeInterop/cinterop/libs/macos-arm64
libraryPaths.macos_x64 = src/nativeInterop/cinterop/libs/macos-x64

# Note: Copy your compiled llama.cpp artifacts to:
# - shared/src/nativeInterop/cinterop/headers/llama.h
# - shared/src/nativeInterop/cinterop/libs/<platform>/libllama.a
