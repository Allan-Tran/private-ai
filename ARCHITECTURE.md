# Private AI Vault - Architecture Documentation

## ğŸ›ï¸ System Architecture

This document describes the architectural decisions, patterns, and design principles used in the Private AI Vault.

---

## ğŸ¯ Core Architectural Principles

### 1. Sovereign Data
- **All data stays on device** - No cloud sync, no external APIs
- **Local inference only** - Models run on the user's hardware
- **Privacy by design** - No telemetry, tracking, or data collection

### 2. Vertical Slice Architecture
- **Feature-based organization** - Each feature is self-contained
- **Full-stack slices** - Data â†’ Domain â†’ UI in one module
- **Independent evolution** - Features can be added/removed without affecting others

### 3. Platform Abstraction
- **Kotlin Multiplatform** - Write once, run on Android/Windows/macOS
- **Platform expectations** - Abstract platform-specific code with `expect/actual`
- **Native interop** - C++ bridge for high-performance inference

---

## ğŸ“ System Layers

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     UI Layer                             â”‚
â”‚  (Compose Multiplatform - Material 3)                    â”‚
â”‚  - SessionAnalystScreen.kt                               â”‚
â”‚  - DocumentVaultScreen.kt                                â”‚
â”‚  - ActiveDeskScreen.kt                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Domain Layer                            â”‚
â”‚  (Business Logic - Use Cases)                            â”‚
â”‚  - SessionAnalystUseCase.kt                              â”‚
â”‚  - DocumentProcessingUseCase.kt                          â”‚
â”‚  - RAG orchestration & prompt building                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Data Layer                             â”‚
â”‚  (Repositories - Data Access)                            â”‚
â”‚  - SessionRepository.kt                                  â”‚
â”‚  - DocumentRepository.kt                                 â”‚
â”‚  - Chunking, embedding coordination                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Inference Engine   â”‚      Vector Store                â”‚
â”‚   (llama.cpp)        â”‚      (SQLite + sqlite-vec)       â”‚
â”‚                      â”‚                                   â”‚
â”‚  - Model loading     â”‚  - Document storage              â”‚
â”‚  - Text generation   â”‚  - Embedding storage             â”‚
â”‚  - Embedding gen.    â”‚  - Similarity search             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Platform Layer (expect/actual)              â”‚
â”‚  - FileSystemAccess                                      â”‚
â”‚  - Platform detection                                    â”‚
â”‚  - Native library loading                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Native Layer                            â”‚
â”‚  - llama.cpp (C++)                                       â”‚
â”‚  - SQLite + vec0 extension                               â”‚
â”‚  - OS-specific APIs                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ—‚ï¸ Module Organization

### Shared Module
**Purpose**: Platform-agnostic code shared across all targets

**Structure**:
```
shared/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ commonMain/          # Pure Kotlin (no platform deps)
â”‚   â”œâ”€â”€ androidMain/         # Android implementations
â”‚   â”œâ”€â”€ desktopMain/         # JVM implementations
â”‚   â””â”€â”€ nativeMain/          # Native (Windows/macOS) implementations
â””â”€â”€ nativeInterop/
    â””â”€â”€ cinterop/
        â”œâ”€â”€ llamacpp.def     # C interop definition
        â””â”€â”€ headers/         # C header files
```

**Key Abstractions**:
- `Platform` - Platform detection
- `FileSystemAccess` - File I/O abstraction

### Core Modules

#### 1. Inference Engine (`core/inference-engine`)
**Purpose**: Bridge between Kotlin and llama.cpp for local LLM inference

**Components**:
- `InferenceEngine` interface - Platform-agnostic API
- `LlamaCppInferenceEngine` - Native implementation using C interop
- `DesktopInferenceEngine` - JVM wrapper (loads native library)

**Key Operations**:
```kotlin
interface InferenceEngine {
    suspend fun loadModel(path: String, params: ModelParams): Boolean
    fun generateStream(prompt: String, params: GenerationParams): Flow<String>
    suspend fun embed(text: String): FloatArray
}
```

**Native Binding**:
```
Kotlin (expect/actual)
    â†“
cinterop (FFI)
    â†“
llama.cpp (C++)
    â†“
GGUF Model Files
```

#### 2. Vector Store (`core/vector-store`)
**Purpose**: RAG storage with semantic search capabilities

**Components**:
- `VectorStore` interface - Platform-agnostic API
- `SqliteVectorStore` - Implementation using SQLDelight
- SQL schema with virtual table for vector search

**Key Operations**:
```kotlin
interface VectorStore {
    suspend fun addDocument(doc: Document, chunks: List<DocumentChunk>)
    suspend fun searchSimilar(embedding: FloatArray, limit: Int): List<SearchResult>
    suspend fun createSession(session: Session)
}
```

**Storage**:
```
SQLite Database
â”œâ”€â”€ documents          # Raw documents
â”œâ”€â”€ chunks             # Text chunks with metadata
â”œâ”€â”€ vec_chunks         # Virtual table (sqlite-vec)
â”œâ”€â”€ sessions           # Active Desk sessions
â””â”€â”€ session_documents  # Many-to-many linking
```

### Feature Modules (Vertical Slices)

#### Session Analyst (`features/session-analyst`)
**Purpose**: Boxing coach training session analysis

**Vertical Structure**:
```
session-analyst/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ SessionRepository.kt       # Data access
â”œâ”€â”€ domain/
â”‚   â”œâ”€â”€ Models.kt                  # Domain entities
â”‚   â””â”€â”€ SessionAnalystUseCase.kt   # Business logic
â””â”€â”€ ui/
    â”œâ”€â”€ SessionAnalystScreen.kt    # Compose UI
    â””â”€â”€ SessionAnalystViewModel.kt # State management
```

**Data Flow**:
```
User Input (UI)
    â†“
ViewModel (state management)
    â†“
Use Case (RAG orchestration)
    â†“
Repository (data coordination)
    â†“ â†˜
VectorStore  InferenceEngine
(context)    (generation)
    â†˜ â†“
   Response (streaming)
```

---

## ğŸ”„ RAG (Retrieval Augmented Generation) Flow

### 1. Indexing Phase (Data Ingestion)

```
User uploads document
    â†“
Repository.addSessionNote()
    â†“
Chunk content (paragraphs/sentences)
    â†“
For each chunk:
    InferenceEngine.embed(chunk) â†’ FloatArray
    â†“
Store in VectorStore
    Document â†’ chunks table
    Embeddings â†’ vec_chunks (virtual table)
```

### 2. Query Phase (Inference with Context)

```
User asks question
    â†“
Use Case.analyzeSession(query)
    â†“
InferenceEngine.embed(query) â†’ FloatArray
    â†“
VectorStore.searchSimilar(queryEmbedding)
    â†“ (cosine similarity search)
Retrieve top K relevant chunks
    â†“
Build prompt with context:
    "Context: [chunk1, chunk2, ...]
     Question: [user query]"
    â†“
InferenceEngine.generateStream(prompt)
    â†“ (streaming tokens)
Display in UI (real-time)
```

### 3. Context Window Management

```kotlin
// Prompt structure
val prompt = """
You are an expert boxing coach assistant.

## Session Context:
${retrievedChunks.joinToString("\n\n")}

## Question:
${userQuestion}

## Analysis:
""".trimIndent()

// Stream generation
inferenceEngine.generateStream(prompt, GenerationParams(
    maxTokens = 1024,
    temperature = 0.7f
))
```

---

## ğŸ”Œ C Interop (Kotlin â†” llama.cpp)

### Binding Generation

**Definition File** (`llamacpp.def`):
```
headers = llama.h
package = com.privateai.vault.llamacpp
compilerOpts = -I/path/to/headers
linkerOpts = -L/path/to/libs -lllama
```

**Gradle Task**:
```bash
./gradlew :shared:cinteropLlamacppWindowsX64
```

**Generated Bindings**:
```kotlin
// Auto-generated by cinterop
@CPointer llama_context
@CPointer llama_model

external fun llama_load_model_from_file(...)
external fun llama_new_context_with_model(...)
external fun llama_eval(...)
```

### Usage in Kotlin

```kotlin
@OptIn(ExperimentalForeignApi::class)
class LlamaCppInferenceEngine : InferenceEngine {
    private var context: CPointer<llama_context>? = null

    override suspend fun loadModel(path: String, params: ModelParams): Boolean {
        return memScoped {
            val modelParams = llama_model_default_params()
            modelParams.n_gpu_layers = params.gpuLayers

            val model = llama_load_model_from_file(path, modelParams)
                ?: return false

            // ... create context, etc.
        }
    }
}
```

---

## ğŸ¨ Active Desk Concept

The **Active Desk** is a workspace where users can temporarily expand the AI's context by dragging files.

### Workflow

```
User drags file â†’ Active Desk
    â†“
File processed (OCR, transcription, etc.)
    â†“
Content chunked & embedded
    â†“
Added to current Session
    â†“
AI context expanded
    â†“
User asks questions â†’ AI uses new context
    â†“
User removes file â†’ Context reduced
```

### Implementation (Planned)

```kotlin
@Composable
fun ActiveDeskWorkspace() {
    var activeFiles by remember { mutableStateOf<List<File>>(emptyList()) }

    DropZone(
        onDrop = { files ->
            files.forEach { file ->
                // Process and add to session
                viewModel.addFileToActiveDesk(file)
            }
            activeFiles += files
        }
    )

    // Display active context
    activeFiles.forEach { file ->
        ActiveFileCard(
            file = file,
            onRemove = { viewModel.removeFileFromActiveDesk(it) }
        )
    }
}
```

---

## ğŸ” Security Architecture

### Data Protection

1. **Encryption at Rest** (Future)
   - SQLCipher for database encryption
   - AES-256 for model file encryption

2. **Memory Protection**
   - Clear sensitive data after use
   - Use `SecureString` for prompts with PII

3. **File System Isolation**
   - All data in app-private directory
   - No world-readable files

### Privacy Guarantees

```
âœ… No network calls (after model download)
âœ… No telemetry
âœ… No crash reporting
âœ… No analytics
âœ… No cloud sync
âœ… No external dependencies at runtime
```

---

## âš¡ Performance Considerations

### Model Quantization

Use quantized models for speed:
- **Q4_K_M**: 4-bit quantization (recommended)
- **Q5_K_M**: 5-bit (better quality)
- **Q8_0**: 8-bit (highest quality)

### Context Window Optimization

```kotlin
// Balance between quality and speed
ModelParams(
    contextSize = 2048,   // Smaller = faster
    batchSize = 512,      // Affects throughput
    threads = 4           // Match CPU cores
)
```

### Vector Search Optimization

```kotlin
// Limit search results
vectorStore.searchSimilar(
    embedding,
    limit = 5,           // Fewer = faster
    threshold = 0.75f    // Higher = more selective
)
```

---

## ğŸ§ª Testing Strategy

### Unit Tests
- Repository logic
- Use case orchestration
- Chunking algorithms
- Prompt building

### Integration Tests
- Inference engine with real models
- Vector store operations
- End-to-end RAG flow

### Platform Tests
- File system access on each platform
- Native library loading
- C interop correctness

---

## ğŸš€ Deployment

### Desktop (Windows/macOS)

**Package Structure**:
```
PrivateAIVault.app/ (or .msi)
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ PrivateAIVault.exe
â”‚   â”œâ”€â”€ native-libs/
â”‚   â”‚   â”œâ”€â”€ llama.dll
â”‚   â”‚   â””â”€â”€ vec0.dll
â”‚   â””â”€â”€ runtime/
â””â”€â”€ models/          # User downloads separately
```

**Installation**:
1. Install application
2. User downloads preferred model
3. First run: configure model path

### Android

**APK Structure**:
```
.apk
â”œâ”€â”€ classes.dex
â”œâ”€â”€ lib/
â”‚   â”œâ”€â”€ arm64-v8a/
â”‚   â”‚   â””â”€â”€ libllama.so
â”‚   â””â”€â”€ armeabi-v7a/
â”‚       â””â”€â”€ libllama.so
â””â”€â”€ assets/
    â””â”€â”€ (no models bundled - downloaded on demand)
```

---

## ğŸ“Š Scalability

### Model Management
- Download models on-demand
- Store multiple models
- Switch between models at runtime

### Vector Store Growth
- Efficient indexing with sqlite-vec
- Automatic cleanup of old sessions
- Configurable retention policies

### Performance Scaling
- CPU: More threads for faster inference
- GPU: Future support for Metal/CUDA
- Memory: Efficient model quantization

---

## ğŸ”® Future Enhancements

### Multi-Modal Support
- Image analysis with vision models
- Audio transcription with Whisper
- Video processing

### Advanced RAG
- Hybrid search (keyword + vector)
- Re-ranking with cross-encoder
- Query expansion

### Platform Expansion
- iOS (Kotlin/Native)
- Linux desktop
- Web (WASM)

---

## ğŸ“š References

- [Kotlin Multiplatform Docs](https://kotlinlang.org/docs/multiplatform.html)
- [llama.cpp GitHub](https://github.com/ggerganov/llama.cpp)
- [sqlite-vec Documentation](https://github.com/asg017/sqlite-vec)
- [Vertical Slice Architecture](https://www.jimmybogard.com/vertical-slice-architecture/)

---

**Last Updated**: 2025-01-07
**Architecture Version**: 1.0
